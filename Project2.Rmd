---
title: "Project2"
author: "Kan Luo"
date: "6/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(readr)
library(tidyverse)
library(GGally)
library(caret)
```

# Introduction section  

## Data background  

Read in data set.
```{r read in data, echo=TRUE, message=FALSE}
ONP_raw <-  read_csv("OnlineNewsPopularity.csv")
```

The data set [**Online News Popularity**](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) inlcudes the features of articles published by [Mashable](www.mashable.com) in two years. There are `r nrow(ONP_raw)` observations and `r ncol(ONP_raw)` variables in the dataset, without any missing values.  


There are:  

 * 2 non-predictive variable: `r names(ONP_raw)[1:2]`  
 * 1 goal variable: `r names(ONP_raw)[length(ONP_raw)]`  
 * `r length(ONP_raw)-3` variables used to predict the goal variable.  


Following are the attributes of all variables provided by the original website:  

`0. url: URL of the article (non-predictive)`  
`1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)`  
`2. n_tokens_title: Number of words in the title`  
`3. n_tokens_content: Number of words in the content`  
`4. n_unique_tokens: Rate of unique words in the content`  
`5. n_non_stop_words: Rate of non-stop words in the content`  
`6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content`  
`7. num_hrefs: Number of links`  
`8. num_self_hrefs: Number of links to other articles published by Mashable`  
`9. num_imgs: Number of images`  
`10. num_videos: Number of videos`  
`11. average_token_length: Average length of the words in the content`  
`12. num_keywords: Number of keywords in the metadata`  
`13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?`  
`14. data_channel_is_entertainment: Is data channel 'Entertainment'?`  
`15. data_channel_is_bus: Is data channel 'Business'?`  
`16. data_channel_is_socmed: Is data channel 'Social Media'?`  
`17. data_channel_is_tech: Is data channel 'Tech'?`  
`18. data_channel_is_world: Is data channel 'World'?`  
`19. kw_min_min: Worst keyword (min. shares)`  
`20. kw_max_min: Worst keyword (max. shares)`  
`21. kw_avg_min: Worst keyword (avg. shares)`  
`22. kw_min_max: Best keyword (min. shares)`  
`23. kw_max_max: Best keyword (max. shares)`  
`24. kw_avg_max: Best keyword (avg. shares)`  
`25. kw_min_avg: Avg. keyword (min. shares)`  
`26. kw_max_avg: Avg. keyword (max. shares)`  
`27. kw_avg_avg: Avg. keyword (avg. shares)`  
`28. self_reference_min_shares: Min. shares of referenced articles in Mashable`  
`29. self_reference_max_shares: Max. shares of referenced articles in Mashable`  
`30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable`  
`31. weekday_is_monday: Was the article published on a Monday?`  
`32. weekday_is_tuesday: Was the article published on a Tuesday?`  
`33. weekday_is_wednesday: Was the article published on a Wednesday?`  
`34. weekday_is_thursday: Was the article published on a Thursday?`  
`35. weekday_is_friday: Was the article published on a Friday?`  
`36. weekday_is_saturday: Was the article published on a Saturday?`  
`37. weekday_is_sunday: Was the article published on a Sunday?`  
`38. is_weekend: Was the article published on the weekend?`  
`39. LDA_00: Closeness to LDA topic 0`  
`40. LDA_01: Closeness to LDA topic 1`  
`41. LDA_02: Closeness to LDA topic 2`  
`42. LDA_03: Closeness to LDA topic 3`  
`43. LDA_04: Closeness to LDA topic 4`  
`44. global_subjectivity: Text subjectivity`  
`45. global_sentiment_polarity: Text sentiment polarity`  
`46. global_rate_positive_words: Rate of positive words in the content`  
`47. global_rate_negative_words: Rate of negative words in the content`  
`48. rate_positive_words: Rate of positive words among non-neutral tokens`  
`49. rate_negative_words: Rate of negative words among non-neutral tokens`  
`50. avg_positive_polarity: Avg. polarity of positive words`  
`51. min_positive_polarity: Min. polarity of positive words`  
`52. max_positive_polarity: Max. polarity of positive words`  
`53. avg_negative_polarity: Avg. polarity of negative words`  
`54. min_negative_polarity: Min. polarity of negative words`  
`55. max_negative_polarity: Max. polarity of negative words`  
`56. title_subjectivity: Title subjectivity`  
`57. title_sentiment_polarity: Title polarity`  
`58. abs_title_subjectivity: Absolute subjectivity level`  
`59. abs_title_sentiment_polarity: Absolute polarity level`  
`60. shares: Number of shares (target)`  


The details about the estimated relative performance values can be found in the [article by K. Fernandes, P. Vinagre and P. Cortez](https://www.researchgate.net/publication/283510525_A_Proactive_Intelligent_Decision_Support_System_for_Predicting_the_Popularity_of_Online_News). The original contents can be found by the address in the `url` variable.  


Data sources provided by:  

 * Kelwin Fernandes (kafc @ inesctec.pt, kelwinfc @ gmail.com) - INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.  
 * Pedro Vinagre (pedro.vinagre.sousa @ gmail.com) - ALGORITMI Research Centre, Universidade do Minho, Portugal  
 * Paulo Cortez - ALGORITMI Research Centre, Universidade do Minho, Portugal
 * Pedro Sernadela - Universidade de Aveiro  

## Purpose of analysis  

The goal is creating models to predict the `shares` variable from the data set with any chosen weekdays (Monday, Tuesday, ect.).  

## Method will use  

The method of prediction is using R to create two models based on training data set:  

 * Linear regression model: multiple linear regression.  
 * Non-linear model: random forests method.  
After validating the two models on the training data set, will use the model to predict the `shares` variable on test data set, and compare the misclassification rate.  

# Data  

To generate analysis report for each weekday, I firstly grab the sub-data set by weekday_is_* variable, and discard two non-predictive variables as well as weekday_is_* variables. Based on the sub-data set, create training data set (70%) and test data set (30%).  

```{r select Monday, echo=TRUE, message=FALSE}
#Create a weekday_is_* include weekday information. 
#ONP_weekday <- ONP_raw %>% mutate( "weekday_is_*" = ifelse(weekday_is_monday == 1, 1, ifelse(weekday_is_tuesday ==1, 2, ifelse(weekday_is_wednesday ==1, 3, ifelse(weekday_is_thursday ==1, 4, ifelse(weekday_is_friday ==1, 5, ifelse(weekday_is_saturday ==1, 6, ifelse(weekday_is_sunday ==1, 7, 8))))))))

#Select Monday data, discard weekday_is_* variables.
data <- ONP_raw %>% filter( weekday_is_monday == 1) %>% 
  select(-url, -timedelta, -weekday_is_monday, 
         -weekday_is_tuesday, -weekday_is_wednesday, -weekday_is_thursday, 
         -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday, -is_weekend)

#Set seed for reproducible.
set.seed(1)

#create train and test data sets. 
train <- sample(1:nrow(data), size = nrow(data)*0.7)

test <- dplyr::setdiff(1:nrow(data), train)

dataTrain <- data[train, ]

dataTest <- data[test, ]
```

This sub-data set contains `r nrow(data)` observations and `r ncol(data)` variables.  
According to `Fernandes`'s article, for random tree method, `keyword` based features are most important for predicting `shares`. It is make sence that keywords can grab reader's attention and interests quickly. The natural language processing features (LDA topics) and previous shares of Mashable linksare also very important. Some other features such as words also have relative high ranking in RF model importance. I will involve all the available predictive variables in both my linear and non-linear models.  

# Summarizations  

Using `summary()` function, look at some basic statistic information on each variabl, get an idea about min, max, mean, 1st/2nd/3rd quantiles.  

```{r summary, echo=TRUE}
summary(dataTrain)
```

Notice shares variable has a very large range, plot it to check.  
```{r shares, echo=TRUE}
histogram(dataTrain$shares)
```

It is right skewed. Use log to make it more normal distributed.  

```{r logshare, echo=TRUE}
dataTrain <- dataTrain %>% mutate(log_shares = log(shares))
histogram(dataTrain$log_shares)
```
The `log(shares)` is better normally distributed, will use it for model fit and predict.  

The following correlation plot shows the correlation among keywords features, natural language processing features and shares. Blue color indicates positive correlated, red color indicates negative correlated, size of dots represents how strong are two variables correlated.  
```{r corr, echo=TRUE, message=FALSE}
library(corrplot)
#Create the correlation sub data set.
data_cor <- dataTrain %>% select(kw_min_min, kw_max_min, kw_avg_min, 
                                 kw_min_max, kw_max_max, kw_avg_max, 
                                 kw_min_avg, kw_max_avg, kw_avg_avg, 
                                 LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, 
                                 shares)
#calculate correlation and plot.
Correlation <- cor(data_cor, method = "spearman")
corrplot(Correlation, type = "upper", tl.pos = "lt")
corrplot(Correlation, type = "lower", method = "number", tl.pos = "n", add = TRUE, diag = FALSE, number.cex = 0.5)

```


We can also take a look at the channel feature variables and the number of shares by channel.  
The count histogram plot of channels shows the number of articles published by each channel on selected weekdays.  
The point plot shows the shares number of articles under each channel. 
```{r channel_sub, echo=TRUE}
#Create new variable channel, and make a sub data set contains channel info and shares.
channel_sub <- dataTrain %>% mutate( channel = ifelse(data_channel_is_lifestyle == 1, "lifestyle", ifelse(data_channel_is_entertainment ==1, "entertainment", ifelse(data_channel_is_bus ==1, "bus", ifelse(data_channel_is_socmed ==1, "socmed", ifelse(data_channel_is_tech ==1, "tech", ifelse(data_channel_is_world ==1, "world", "NA"))))))) %>% select(channel, shares)
#Make plot on counts of each channel numbers
g1 <- ggplot(channel_sub, aes(x = channel))
g1 + stat_count() + ggtitle("Number of articles on each channel")
#Make plot on shares values by channels.
g2 <- ggplot(channel_sub, aes(x = channel, y=shares))
g2 + geom_point(aes(fill = channel, colour = channel),position = "jitter") + ggtitle("Shares by Channels")

```


# Modeling  

## Random Forests
```{r RF, echo=TRUE}
#Subset a smaller dataTrain dataset for fitting.

dataTrain <- dataTrain[1:300,]

#remove the shares column, use log_shares to train and predict
dataTrain <- dataTrain %>% select(-shares)
#setup the control
trctrl <- trainControl(method = "repeatedcv", number = 3, repeats = 1)
#Fit a RF tree
rf_fit<- train(log_shares ~ ., data=dataTrain, 
                method = "rf", trControl = trctrl, 
                preProcess = c("center", "scale"), tuneLength=3)
#check the fit
rf_fit
#plot the fit
plot(rf_fit)
#plot the model
plot(rf_fit$finalModel)

#predict() using the titanicDataTest dataset
rf_pred <- predict(rf_fit, newdata = dataTest)
#Use exp to convert the log value back.
rf_pred_e <- exp(rf_pred)

#Use ifelse to convert shares variable into 0 ( < 1400) or 1 ( >=1400)
#Turn them into factor.
#Use confusionMatrix function to compare the accuracy.
rf_match <- confusionMatrix(as.factor(ifelse(rf_pred_e < 1400, 0, 1)), as.factor(ifelse(dataTest$shares < 1400, 0, 1)))
#Calculate miss rate
rf_miss <- unname(1-rf_match[[3]][1])
#Call miss rate
rf_miss
```

## Linear regression model  

Create following linear regression models:  
```{r linear, echo=TRUE}
lin_fit1 <- lm(log_shares ~ ., data = dataTrain)
lin_fit2 <- lm(log_shares ~ kw_min_min + kw_max_min + kw_avg_min + 
                 kw_min_max + kw_max_max + kw_avg_max + kw_min_avg + 
                 kw_max_avg + kw_avg_avg, data = dataTrain)
lin_fit3 <- lm(log_shares ~ kw_min_min + kw_max_min + kw_avg_min + 
                 kw_min_max + kw_max_max + kw_avg_max + kw_min_avg + 
                 kw_max_avg + kw_avg_avg + LDA_00 + LDA_01 + LDA_02 + 
                 LDA_03 + LDA_04, data = dataTrain)

```

Use [Dr. Post's](https://www4.stat.ncsu.edu/~post/) function for Adj R Square, AIC, AICc, BIC comparsion.   
```{r compareFitStats, echo=TRUE, message=FALSE}
compareFitStats <- function(fit1, fit2, fit3){
	require(MuMIn)
	fitStats <- data.frame(fitStat = c("Adj R Square", "AIC", "AICc", "BIC"),
	    col1 = round(c(summary(fit1)$adj.r.squared, AIC(fit1), 
									MuMIn::AICc(fit1), BIC(fit1)), 3),
	    col2 = round(c(summary(fit2)$adj.r.squared, AIC(fit2), 
									MuMIn::AICc(fit2), BIC(fit2)), 3),
			col3 = round(c(summary(fit3)$adj.r.squared, AIC(fit3), 
									MuMIn::AICc(fit3), BIC(fit3)), 3))
	#put names on returned df
	calls <- as.list(match.call())
	calls[[1]] <- NULL
	names(fitStats)[2:4] <- unlist(calls)
	fitStats
}

compareFitStats(lin_fit1, lin_fit2, lin_fit3)
```

This function can calculate the Adj R Square, AIC, AICc, BIC statistics for each fit, and export a table to easy compare. Typically, we will prefer the fits which has larger Adj R Square, and smaller AIC, AICc AND BIC.  

Will choose lin_fit3 to do the prediction.  

```{r linear pred, echo=TRUE, message=FALSE, warning=FALSE}
lin_pred <- predict(lin_fit3, newdata = dataTest)
lin_pred_e <- exp(lin_pred)
lin_match <- confusionMatrix(as.factor(ifelse(lin_pred_e < 1400, 0, 1)), as.factor(ifelse(dataTest$shares < 1400, 0, 1)))
#Calculate miss rate
lin_miss <- unname(1-lin_match[[3]][1])
#Call miss rate
lin_miss
```

## Compare model  

```{r compare, echo=TRUE}
table(rf_miss, lin_miss)
```

The lower missaccuracy rate is `r min(rf_miss, lin_miss)`
